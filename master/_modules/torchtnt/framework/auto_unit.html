


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchtnt.framework.auto_unit &mdash; TorchTNT master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

  
  
    <div id="redirect-banner" style="display: none">
      <p>
        ðŸŽ‰ This is the public documentation. There is internal documentation for Meta employees at
        <a href="https://www.internalfb.com/intern/staticdocs/torchtnt/">https://www.internalfb.com/intern/staticdocs/torchtnt/</a>
      </p>
    </div>
  

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (unstable)
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpointing.html">Checkpointing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/unit.html">Unit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/auto_unit.html">AutoUnit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/eval.html">Evaluate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/predict.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/fit.html">Fit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/state.html">State</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/callbacks.html">Callbacks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../utils/utils.html">Utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchtnt.framework.auto_unit</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchtnt.framework.auto_unit</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="c1"># pyre-strict</span>


<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">ContextManager</span><span class="p">,</span>
    <span class="n">Generic</span><span class="p">,</span>
    <span class="n">Iterator</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">TypeVar</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">pyre_extensions</span> <span class="kn">import</span> <span class="n">none_throws</span>
<span class="kn">from</span> <span class="nn">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span> <span class="nn">torch.optim.swa_utils</span> <span class="kn">import</span> <span class="n">SWALR</span>
<span class="kn">from</span> <span class="nn">torchtnt.framework._unit_utils</span> <span class="kn">import</span> <span class="n">_step_requires_iterator</span>
<span class="kn">from</span> <span class="nn">torchtnt.framework.state</span> <span class="kn">import</span> <span class="n">ActivePhase</span><span class="p">,</span> <span class="n">EntryPoint</span><span class="p">,</span> <span class="n">State</span>
<span class="kn">from</span> <span class="nn">torchtnt.framework.unit</span> <span class="kn">import</span> <span class="n">EvalUnit</span><span class="p">,</span> <span class="n">PredictUnit</span><span class="p">,</span> <span class="n">TPredictData</span><span class="p">,</span> <span class="n">TrainUnit</span>
<span class="kn">from</span> <span class="nn">torchtnt.framework.utils</span> <span class="kn">import</span> <span class="n">get_timing_context</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.device</span> <span class="kn">import</span> <span class="n">copy_data_to_device</span><span class="p">,</span> <span class="n">record_data_in_stream</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.env</span> <span class="kn">import</span> <span class="n">init_from_env</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.lr_scheduler</span> <span class="kn">import</span> <span class="n">TLRScheduler</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.precision</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_precision_str_to_dtype</span><span class="p">,</span>
    <span class="n">get_grad_scaler_from_precision</span><span class="p">,</span>
    <span class="n">GradScaler</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.prepare_module</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_is_fsdp_module</span><span class="p">,</span>
    <span class="n">ActivationCheckpointParams</span><span class="p">,</span>
    <span class="n">FSDPStrategy</span><span class="p">,</span>
    <span class="n">prepare_fsdp</span><span class="p">,</span>
    <span class="n">prepare_module</span><span class="p">,</span>
    <span class="n">Strategy</span><span class="p">,</span>
    <span class="n">TorchCompileParams</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torchtnt.utils.swa</span> <span class="kn">import</span> <span class="n">AveragedModel</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="n">_logger</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="n">TData</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;TData&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SWALRParams</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dataclass to store parameters for SWALR learning rate scheduler.</span>
<span class="sd">    See https://github.com/pytorch/pytorch/blob/main/torch/optim/swa_utils.py#L279</span>
<span class="sd">    for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        anneal_steps_or_epochs: number of steps or epochs to anneal the SWA Scheduler</span>
<span class="sd">        anneal_strategy: method for annealing, supports &quot;linear&quot; and &quot;cos&quot;</span>
<span class="sd">        swa_lrs: the learning rate value for all param groups together or separately for each group</span>

<span class="sd">        Note: Whether steps or epochs is used based on what `step_lr_interval` is set on the AutoUnit.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">anneal_steps_or_epochs</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">anneal_strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span>
    <span class="n">swa_lrs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.05</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SWAParams</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dataclass to store parameters for stochastic weight averaging.</span>

<span class="sd">    Args:</span>
<span class="sd">        warmup_steps_or_epochs: number of steps or epochs before starting SWA</span>
<span class="sd">        step_or_epoch_update_freq: number of steps or epochs between each SWA update</span>
<span class="sd">        use_buffers: if ``True``, it will compute running averages for</span>
<span class="sd">            both the parameters and the buffers of the model. (default: ``True``)</span>
<span class="sd">            This will update activation statistics for Batch Normalization. This is an</span>
<span class="sd">            alternative to calling `torch.optim.swa_utils.update_bn` post-training.</span>
<span class="sd">        averaging_method: whether to use SWA or EMA to average model weights</span>
<span class="sd">        ema_decay:  the exponential decay applied to the averaged parameters. This param</span>
<span class="sd">            is only needed for EMA, and is ignored otherwise (for SWA).</span>
<span class="sd">        use_lit: if True, will use Lit EMA style by adjusting weight decay based on the</span>
<span class="sd">            number of updates. The EMA decay will start small and will approach the</span>
<span class="sd">            specified ema_decay as more updates occur. The ``averaging_method`` must be</span>
<span class="sd">            set to ema.</span>
<span class="sd">        swalr_params: params for SWA learning rate scheduler</span>

<span class="sd">        Note: Whether steps or epochs is used based on what `step_lr_interval` is set on the AutoUnit.</span>

<span class="sd">        Note: Only one of avg_fn, multi_avg_fn should be specified</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">warmup_steps_or_epochs</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">step_or_epoch_update_freq</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">use_buffers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">averaging_method</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;ema&quot;</span><span class="p">,</span> <span class="s2">&quot;swa&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;ema&quot;</span>
    <span class="n">ema_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span>
    <span class="n">use_lit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">swalr_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SWALRParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainStepResults</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dataclass to store training step results.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss: the loss computed in the ``compute_loss`` function</span>
<span class="sd">        total_grad_norm: total norm of the parameter gradients, if gradient norm clipping is enabled</span>
<span class="sd">        outputs: the outputs of the model forward pass</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">total_grad_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="c1"># pyre-fixme[4]: Attribute `outputs` of class `TrainStepResults` must have a type other than `Any`.</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span>


<span class="k">class</span> <span class="nc">_ConfigureOptimizersCaller</span><span class="p">(</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="c1"># pyre-fixme[3]: Return type must be annotated.</span>
    <span class="c1"># pyre-fixme[2]: Parameter must be annotated.</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">x</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">x</span><span class="o">.</span><span class="n">swa_scheduler</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">configure_optimizers_and_lr_scheduler</span><span class="p">(</span>
                <span class="n">x</span><span class="o">.</span><span class="n">module</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">swa_params</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">swa_params</span><span class="o">.</span><span class="n">swalr_params</span><span class="p">:</span>
                <span class="n">swalr_params</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">swa_params</span><span class="o">.</span><span class="n">swalr_params</span>
                <span class="n">x</span><span class="o">.</span><span class="n">swa_scheduler</span> <span class="o">=</span> <span class="n">SWALR</span><span class="p">(</span>
                    <span class="n">optimizer</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
                    <span class="n">swa_lr</span><span class="o">=</span><span class="n">swalr_params</span><span class="o">.</span><span class="n">swa_lrs</span><span class="p">,</span>
                    <span class="n">anneal_epochs</span><span class="o">=</span><span class="n">swalr_params</span><span class="o">.</span><span class="n">anneal_steps_or_epochs</span><span class="p">,</span>
                    <span class="n">anneal_strategy</span><span class="o">=</span><span class="n">swalr_params</span><span class="o">.</span><span class="n">anneal_strategy</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">_AutoUnitMixin</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">TData</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A mixin to share initialization of shared attributes and introduce prefetching.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">detect_anomaly</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">torch_compile_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchCompileParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_prefetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="n">init_from_env</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">convert_precision_str_to_dtype</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">precision</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">detect_anomaly</span> <span class="o">=</span> <span class="n">detect_anomaly</span>

        <span class="c1"># create autocast context based on precision and device type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maybe_autocast_precision</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span>
            <span class="n">device_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># cuda stream to use for moving data to device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">Stream</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="n">enable_prefetch</span><span class="p">)</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="c1"># dict mapping phase to whether the next batch which has been prefetched for that phase and is ready to be used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_next_batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">ActivePhase</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TData</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">EVALUATE</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">PREDICT</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># dict mapping phase to whether the next batch for that phase has been prefetched and is ready to be used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_prefetched</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">ActivePhase</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">EVALUATE</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">ActivePhase</span><span class="o">.</span><span class="n">PREDICT</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="c1"># whether the current batch is the last train batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_last_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_prefetch</span> <span class="o">=</span> <span class="n">enable_prefetch</span>

    <span class="k">def</span> <span class="nf">move_data_to_device</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TData</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The user can override this method with custom code to copy data to device. This will be called at the start of every ``train_step``/``eval_step``/``predict_step``.</span>
<span class="sd">        By default this uses the utility function :py:func:`~torchtnt.utils.copy_data_to_device`.</span>

<span class="sd">        If on GPU, this method will be called on a separate CUDA stream.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``train_step``/``eval_step``/``predict_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``train_step``/``eval_step``/``predict_step``</span>
<span class="sd">            non_blocking: parameter to pass to ``torch.tensor.to``</span>

<span class="sd">        Returns:</span>
<span class="sd">            A batch of data which is on the device</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">copy_data_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_prefetch_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Prefetch the next batch on a separate CUDA stream.&quot;&quot;&quot;</span>
        <span class="n">active_phase</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">active_phase</span>
        <span class="n">phase</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">active_phase</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">phase</span><span class="si">}</span><span class="s2">.next(data_iter)&quot;</span>
            <span class="p">):</span>
                <span class="n">next_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">data_iter</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_next_batch</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_last_batch</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">return</span>

        <span class="n">non_blocking</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_prefetched</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># if on cpu, self._prefetch_stream is None so the torch.cuda.stream call is a no-op</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_stream</span><span class="p">),</span> <span class="n">get_timing_context</span><span class="p">(</span>
            <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">phase</span><span class="si">}</span><span class="s2">.move_data_to_device&quot;</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_next_batch</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">move_data_to_device</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span> <span class="n">next_batch</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_next_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">TData</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_prefetch</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">move_data_to_device</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">active_phase</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">active_phase</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_prefetched</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_prefetched</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_stream</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.wait_stream&quot;</span><span class="p">):</span>
                <span class="c1"># wait on the CUDA stream to complete the host to device copy</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_stream</span><span class="p">)</span>

        <span class="c1"># get the next batch which was stored by _prefetch_next_batch</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_next_batch</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_phase_to_prefetched</span><span class="p">[</span><span class="n">active_phase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_is_last_batch</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_stream</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.record_data_in_stream&quot;</span>
            <span class="p">):</span>
                <span class="c1"># record the batch in the current stream</span>
                <span class="n">record_data_in_stream</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

        <span class="c1"># prefetch the next batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">batch</span>


<div class="viewcode-block" id="AutoPredictUnit"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoPredictUnit">[docs]</a><span class="k">class</span> <span class="nc">AutoPredictUnit</span><span class="p">(</span><span class="n">_AutoUnitMixin</span><span class="p">[</span><span class="n">TPredictData</span><span class="p">],</span> <span class="n">PredictUnit</span><span class="p">[</span><span class="n">TPredictData</span><span class="p">]):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Strategy</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">torch_compile_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchCompileParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">detect_anomaly</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_prefetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        AutoPredictUnit is a convenience for users who are running inference and would like to have certain features handled for them, such as:</span>
<span class="sd">        - Moving data to the correct device.</span>
<span class="sd">        - Running inference under a mixed precision context.</span>
<span class="sd">        - Handling data parallel replication, especially if the module cannot fit on a single device using FullyShardedDataParallel.</span>
<span class="sd">        - Profiling the data transfer to device and forward pass.</span>
<span class="sd">        - Interleaving moving the next batch to the device with running the module&#39;s forward pass on the current batch.</span>

<span class="sd">        Additionally, the AutoPredictUnit offers an optional hook ``on_predict_step_end`` to further post-process module outputs if desired.</span>

<span class="sd">        Then use with the :py:func:`~torchtnt.framework.predict` entry point.</span>

<span class="sd">        For more advanced customization, directly use the :class:`~torchtnt.framework.unit.PredictUnit` interface.</span>

<span class="sd">        Args:</span>
<span class="sd">            module: module to be used during prediction.</span>
<span class="sd">            device: the device to be used.</span>
<span class="sd">            precision: the precision to use in training, as either a string or a torch.dtype.</span>
<span class="sd">            strategy: the data parallelization strategy to be used. if a string, must be one of ``ddp`` or ``fsdp``.</span>
<span class="sd">            torch_compile_params: params for Torch compile https://pytorch.org/docs/stable/generated/torch.compile.html</span>
<span class="sd">            detect_anomaly: whether to enable anomaly detection for the autograd engine https://pytorch.org/docs/stable/autograd.html#anomaly-detection</span>

<span class="sd">        Note:</span>
<span class="sd">            Torch compile support is only available in PyTorch 2.0 or higher.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">torch_compile_params</span><span class="o">=</span><span class="n">torch_compile_params</span><span class="p">,</span>
            <span class="n">detect_anomaly</span><span class="o">=</span><span class="n">detect_anomaly</span><span class="p">,</span>
            <span class="n">enable_prefetch</span><span class="o">=</span><span class="n">enable_prefetch</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">prepare_module</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
            <span class="n">torch_compile_params</span><span class="o">=</span><span class="n">torch_compile_params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># pyre-fixme[3]: Return annotation cannot be `Any`.</span>
<div class="viewcode-block" id="AutoPredictUnit.predict_step"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoPredictUnit.predict_step">[docs]</a>    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TPredictData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># if detect_anomaly is true, run forward pass under detect_anomaly context</span>
        <span class="n">detect_anomaly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">detect_anomaly</span>
        <span class="n">maybe_detect_anomaly</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="n">detect_anomaly</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">detect_anomaly</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">maybe_autocast_precision</span><span class="p">,</span> <span class="n">maybe_detect_anomaly</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.forward&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_predict_step_end</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span></div>

<div class="viewcode-block" id="AutoPredictUnit.on_predict_step_end"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoPredictUnit.on_predict_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_predict_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">TPredictData</span><span class="p">,</span>
        <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># pyre-fixme[2]: Parameter annotation cannot be `Any`.</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will be called at the end of every ``predict_step`` before returning. The user can implement this method with code to update and log their metrics,</span>
<span class="sd">        or do anything else.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``predict_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``predict_step``</span>
<span class="sd">            step: how many ``predict_step`` s have been completed</span>
<span class="sd">            outputs: the outputs of the model forward pass</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="AutoPredictUnit.get_next_predict_batch"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoPredictUnit.get_next_predict_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_next_predict_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TPredictData</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Iterator</span><span class="p">[</span><span class="n">TPredictData</span><span class="p">],</span> <span class="n">TPredictData</span><span class="p">]:</span>
        <span class="c1"># Override the default behavior from PredictUnit in order to enable prefetching if possible.</span>
        <span class="n">pass_data_iter_to_step</span> <span class="o">=</span> <span class="n">_step_requires_iterator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pass_data_iter_to_step</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data_iter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="AutoUnit"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit">[docs]</a><span class="k">class</span> <span class="nc">AutoUnit</span><span class="p">(</span>
    <span class="n">_AutoUnitMixin</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span>
    <span class="n">TrainUnit</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span>
    <span class="n">EvalUnit</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span>
    <span class="n">PredictUnit</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span>
    <span class="n">metaclass</span><span class="o">=</span><span class="n">_ConfigureOptimizersCaller</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The AutoUnit is a convenience for users who are training with stochastic gradient descent and would like to have model optimization</span>
<span class="sd">    and data parallel replication handled for them.</span>
<span class="sd">    The AutoUnit subclasses :class:`~torchtnt.framework.unit.TrainUnit`, :class:`~torchtnt.framework.unit.EvalUnit`, and</span>
<span class="sd">    :class:`~torchtnt.framework.unit.PredictUnit` and implements the ``train_step``, ``eval_step``, and ``predict_step`` methods for the user.</span>

<span class="sd">    For the ``train_step`` it runs:</span>

<span class="sd">    - forward pass and loss computation</span>
<span class="sd">    - backward pass</span>
<span class="sd">    - optimizer step</span>

<span class="sd">    For the ``eval_step`` it only runs forward and loss computation.</span>

<span class="sd">    For the ``predict_step`` it only runs forward computation.</span>

<span class="sd">    To benefit from the AutoUnit, the user must subclass it and implement the ``compute_loss`` and ``configure_optimizers_and_lr_scheduler`` methods.</span>
<span class="sd">    Additionally, the AutoUnit offers these optional hooks:</span>

<span class="sd">    - ``on_train_step_end``</span>
<span class="sd">    - ``on_eval_step_end``</span>
<span class="sd">    - ``on_predict_step_end``</span>

<span class="sd">    The user can also override the LR step method, ``step_lr_scheduler``, in case they want to have custom logic.</span>

<span class="sd">    Then use with the :py:func:`~torchtnt.framework.train`, :py:func:`~torchtnt.framework.evaluate`, :py:func:`~torchtnt.framework.fit`, or</span>
<span class="sd">    :py:func:`~torchtnt.framework.predict` entry point as normal.</span>

<span class="sd">    For more advanced customization, directly use the :class:`~torchtnt.framework.unit.TrainUnit`, :class:`~torchtnt.framework.unit.EvalUnit`,</span>
<span class="sd">    and :class:`~torchtnt.framework.unit.PredictUnit` interfaces.</span>

<span class="sd">    Args:</span>
<span class="sd">        module: module to be used during training/evaluation.</span>
<span class="sd">        device: the device to be used.</span>
<span class="sd">        strategy: the data parallelization strategy to be used. if a string, must be one of ``ddp`` or ``fsdp``.</span>
<span class="sd">        step_lr_interval: whether to step lr_scheduler every step or every epoch. Defaults to every epoch.</span>
<span class="sd">        precision: the precision to use in training/evaluation (using automatic mixed precision), as either a string or a torch.dtype. Acceptable strings are ``&#39;fp32&#39;``, ``&#39;fp16&#39;``, and ``&#39;bf16&#39;``.</span>
<span class="sd">        gradient_accumulation_steps: how many batches to accumulate gradients over.</span>
<span class="sd">        detect_anomaly: whether to enable anomaly detection for the autograd engine https://pytorch.org/docs/stable/autograd.html#anomaly-detection</span>
<span class="sd">        clip_grad_norm: max norm of the gradients for clipping https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html</span>
<span class="sd">        clip_grad_value: max value of the gradients for clipping https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html</span>
<span class="sd">        swa_params: params for stochastic weight averaging https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging</span>
<span class="sd">        torch_compile_params: params for Torch compile https://pytorch.org/docs/stable/generated/torch.compile.html</span>
<span class="sd">        activation_checkpoint_params: params for enabling activation checkpointing</span>
<span class="sd">        training: if True, the optimizer and optionally LR scheduler will be created after the class is initialized.</span>
<span class="sd">        enable_compiled_autograd: if True, `compiled_autograd` will be used to compile the backward, this is an experimental flag.</span>
<span class="sd">        loss_backward_retain_graph:  If ``None`` or ``False``, the graph used to compute</span>
<span class="sd">            the grads will be freed during loss backward pass. Note that in nearly all cases setting</span>
<span class="sd">            this option to True is not needed and often can be worked around</span>
<span class="sd">            in a much more efficient way.</span>
<span class="sd">        enable_prefetch: if True, the data will be prefetched to the device before the next batch is loaded</span>

<span class="sd">    Note:</span>
<span class="sd">        Certain strategies, like :class:`~torchtnt.utils.prepare_module.FSDPStrategy` also support mixed precision as an argument, so can be configured through that class as well.</span>

<span class="sd">    Note:</span>
<span class="sd">        If :class:`~torchtnt.utils.prepare_module.FSDPStrategy` and SWAParams are passed in, the swa model will be sharded with the same FSDP parameters.</span>

<span class="sd">    Note:</span>
<span class="sd">        Torch compile support is only available in PyTorch 2.0 or higher.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strategy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Strategy</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">step_lr_interval</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">detect_anomaly</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_grad_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">swa_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SWAParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">torch_compile_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TorchCompileParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation_checkpoint_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ActivationCheckpointParams</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">enable_compiled_autograd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">loss_backward_retain_graph</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_prefetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">detect_anomaly</span><span class="o">=</span><span class="n">detect_anomaly</span><span class="p">,</span>
            <span class="n">torch_compile_params</span><span class="o">=</span><span class="n">torch_compile_params</span><span class="p">,</span>
            <span class="n">enable_prefetch</span><span class="o">=</span><span class="n">enable_prefetch</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">gradient_accumulation_steps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;gradient_accumulation_steps must be &gt; 0. Got </span><span class="si">{</span><span class="n">gradient_accumulation_steps</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">swa_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SWAParams</span><span class="p">]</span> <span class="o">=</span> <span class="n">swa_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swa_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AveragedModel</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">swa_params</span> <span class="ow">and</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">module_for_swa</span> <span class="o">=</span> <span class="n">module</span>
            <span class="n">skip_deepcopy</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">strategy</span><span class="p">,</span> <span class="n">FSDPStrategy</span><span class="p">):</span>
                <span class="c1"># must use exact same FSDPStrategy as original module</span>
                <span class="c1"># so each rank can computes EMA for its own local shard</span>
                <span class="c1"># since models are sharded identically if FSDP params are the same</span>
                <span class="n">module_for_swa</span> <span class="o">=</span> <span class="n">prepare_fsdp</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">strategy</span><span class="p">)</span>
                <span class="n">skip_deepcopy</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">swa_model</span> <span class="o">=</span> <span class="n">AveragedModel</span><span class="p">(</span>
                <span class="n">module_for_swa</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">use_buffers</span><span class="o">=</span><span class="n">swa_params</span><span class="o">.</span><span class="n">use_buffers</span><span class="p">,</span>
                <span class="n">averaging_method</span><span class="o">=</span><span class="n">swa_params</span><span class="o">.</span><span class="n">averaging_method</span><span class="p">,</span>
                <span class="n">ema_decay</span><span class="o">=</span><span class="n">swa_params</span><span class="o">.</span><span class="n">ema_decay</span><span class="p">,</span>
                <span class="n">skip_deepcopy</span><span class="o">=</span><span class="n">skip_deepcopy</span><span class="p">,</span>
                <span class="n">use_lit</span><span class="o">=</span><span class="n">swa_params</span><span class="o">.</span><span class="n">use_lit</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">prepare_module</span><span class="p">(</span>
            <span class="n">module</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
            <span class="n">torch_compile_params</span><span class="o">=</span><span class="n">torch_compile_params</span><span class="p">,</span>
            <span class="n">activation_checkpoint_params</span><span class="o">=</span><span class="n">activation_checkpoint_params</span><span class="p">,</span>
            <span class="n">enable_compiled_autograd</span><span class="o">=</span><span class="n">enable_compiled_autograd</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GradScaler</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span> <span class="o">=</span> <span class="n">get_grad_scaler_from_precision</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
                <span class="n">is_fsdp_module</span><span class="o">=</span><span class="n">_is_fsdp_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step_lr_interval</span> <span class="o">=</span> <span class="n">step_lr_interval</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">=</span> <span class="n">gradient_accumulation_steps</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span> <span class="o">=</span> <span class="n">clip_grad_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_value</span> <span class="o">=</span> <span class="n">clip_grad_value</span>

        <span class="c1"># create autocast context based on precision and device type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">enable_compiled_autograd</span> <span class="o">=</span> <span class="n">enable_compiled_autograd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_backward_retain_graph</span> <span class="o">=</span> <span class="n">loss_backward_retain_graph</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TLRScheduler</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swa_scheduler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SWALR</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_module_attr</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_module_attr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The AutoUnit is designed to manage the input model using the `self.module` attribute,</span>
<span class="sd">        which should not be reassigned. Additionally, if a subclass saves another attribute</span>
<span class="sd">        referencing the same model instance (wrapped or unwrapped), then the same instance will</span>
<span class="sd">        appear two times in the tracked_modules. This is problematic for checkpointing and handling</span>
<span class="sd">        of evaluation/training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># First time the module attribute is set is in the AutoUnit&#39;s initialization</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;module&quot;</span><span class="p">):</span>
            <span class="k">return</span>

        <span class="c1"># Value of self.module should not be changed after initialization</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;module&quot;</span><span class="p">:</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The self.module attribute is managed by AutoUnit and is not meant to be reassigned.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Otherwise, double check that this is not a duplicate reference to the self.module instance</span>
        <span class="n">managed_modules</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">DDP</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">,</span> <span class="n">FSDP</span><span class="p">):</span>
            <span class="n">managed_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">module</span> <span class="ow">is</span> <span class="n">managed_module</span> <span class="k">for</span> <span class="n">managed_module</span> <span class="ow">in</span> <span class="n">managed_modules</span><span class="p">):</span>
            <span class="n">_logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Attribute &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; of the custom TNT Unit stores a reference to the model managed&quot;</span>
                <span class="o">+</span> <span class="s2">&quot;by AutoUnit. This is known to cause errors on checkpointing and model training &quot;</span>
                <span class="o">+</span> <span class="s2">&quot;mode. Please remove this attribute and access the existing `self.module` instead.&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="AutoUnit.configure_optimizers_and_lr_scheduler"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.configure_optimizers_and_lr_scheduler">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">configure_optimizers_and_lr_scheduler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TLRScheduler</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The user should implement this method with their optimizer and learning rate scheduler construction code. This will be called upon initialization of</span>
<span class="sd">        the AutoUnit.</span>

<span class="sd">        Args:</span>
<span class="sd">            module: the module with which to construct optimizer and lr_scheduler</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing optimizer and optionally the learning rate scheduler</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span></div>

<div class="viewcode-block" id="AutoUnit.compute_loss"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.compute_loss">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="c1"># pyre-fixme[3]: Return annotation cannot contain `Any`.</span>
    <span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The user should implement this method with their loss computation. This will be called every ``train_step``/``eval_step``.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``train_step``/``eval_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``train_step``/``eval_step``</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple containing the loss and the output of the model</span>

<span class="sd">        Note:</span>
<span class="sd">            The module&#39;s forward pass must be run as part of this method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span></div>

    <span class="c1"># pyre-fixme[3]: Return annotation cannot contain `Any`.</span>
<div class="viewcode-block" id="AutoUnit.train_step"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.train_step">[docs]</a>    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">should_update_weights</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_steps_completed_in_epoch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_last_batch</span>

        <span class="c1"># for pyre, assign to local variable</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>

        <span class="c1"># if using gradient accumulation with either DDP or FSDP, when in a step where we will not update the weights,</span>
        <span class="c1"># run forward and backward in no_sync context</span>
        <span class="c1"># https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html#DistributedDataParallel.no_sync</span>
        <span class="c1"># https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.FullyShardedDataParallel.no_sync</span>
        <span class="n">maybe_no_sync</span> <span class="o">=</span> <span class="p">(</span>
            <span class="c1"># pyre-fixme[29]: `Union[Tensor, Module]` is not a function.</span>
            <span class="n">module</span><span class="o">.</span><span class="n">no_sync</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">should_update_weights</span>
            <span class="ow">and</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">DDP</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_fsdp_module</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
            <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="c1"># if detect_anomaly is true, run forward and backward pass in detect_anomaly context</span>
        <span class="n">detect_anomaly</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">detect_anomaly</span>
        <span class="n">maybe_detect_anomaly</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="n">detect_anomaly</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">detect_anomaly</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">grad_scaler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span>
        <span class="k">with</span> <span class="n">maybe_no_sync</span><span class="p">,</span> <span class="n">maybe_detect_anomaly</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">maybe_autocast_precision</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.compute_loss&quot;</span>
                <span class="p">):</span>
                    <span class="c1"># Run the forward pass and compute the loss</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

            <span class="c1"># normalize loss to account for gradient accumulation</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">torch._dynamo.utils</span> <span class="kn">import</span> <span class="n">maybe_enable_compiled_autograd</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">maybe_enable_compiled_autograd</span><span class="p">(</span>
                    <span class="n">val</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[24]: Generic type `ContextManager` expects 1 type</span>
                    <span class="c1">#  parameter.</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ContextManager</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>

            <span class="k">with</span> <span class="n">maybe_enable_compiled_autograd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">enable_compiled_autograd</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">grad_scaler</span><span class="p">:</span>
                    <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">grad_scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
                    <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                        <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.backward&quot;</span>
                    <span class="p">):</span>
                        <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
                            <span class="n">retain_graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_backward_retain_graph</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                        <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.backward&quot;</span>
                    <span class="p">):</span>
                        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_backward_retain_graph</span><span class="p">)</span>

        <span class="n">total_grad_norm</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">should_update_weights</span><span class="p">:</span>
            <span class="n">total_grad_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_weights</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">TrainStepResults</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">total_grad_norm</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_train_step_end</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span></div>

<div class="viewcode-block" id="AutoUnit.on_train_step_end"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.on_train_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_train_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">,</span>
        <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">TrainStepResults</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will be called at the end of every ``train_step`` before returning. The user can implement this method with code to update and log their metrics,</span>
<span class="sd">        or do anything else.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``train_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``train_step``</span>
<span class="sd">            step: how many ``train_step`` s have been completed</span>
<span class="sd">            results: dataclass containing loss, total gradient norm, and outputs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="AutoUnit.on_train_epoch_end"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.on_train_epoch_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note: if overriding ``on_train_epoch_end``, remember to call ``super().on_train_epoch_end()``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_lr_interval</span> <span class="o">==</span> <span class="s2">&quot;epoch&quot;</span><span class="p">:</span>
            <span class="c1"># number of epochs is incremented before calling this, so we&#39;re offsetting by 1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_lr_and_swa</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_epochs_completed</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_is_last_batch</span> <span class="o">=</span> <span class="kc">False</span></div>

    <span class="c1"># pyre-fixme[3]: Return annotation cannot contain `Any`.</span>
<div class="viewcode-block" id="AutoUnit.eval_step"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.eval_step">[docs]</a>    <span class="k">def</span> <span class="nf">eval_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">maybe_autocast_precision</span><span class="p">:</span>
            <span class="c1"># users must override this</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.compute_loss&quot;</span><span class="p">):</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">entry_point</span> <span class="o">==</span> <span class="n">EntryPoint</span><span class="o">.</span><span class="n">FIT</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">on_eval_step_end</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span></div>

<div class="viewcode-block" id="AutoUnit.on_eval_step_end"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.on_eval_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_eval_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">,</span>
        <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="c1"># pyre-fixme[2]: Parameter annotation cannot be `Any`.</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will be called at the end of every ``eval_step`` before returning. The user can implement this method with code to update and log their metrics,</span>
<span class="sd">        or do anything else.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``eval_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``eval_step``</span>
<span class="sd">            step: how many steps have been completed (``train_step`` s when running fit and ``eval_step`` s when running evaluation)</span>
<span class="sd">            loss: the loss computed in the ``compute_loss`` function</span>
<span class="sd">            outputs: the outputs of the model forward pass</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

    <span class="c1"># pyre-fixme[3]: Return annotation cannot contain `Any`.</span>
<div class="viewcode-block" id="AutoUnit.predict_step"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.predict_step">[docs]</a>    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">maybe_autocast_precision</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.forward&quot;</span><span class="p">):</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>
        <span class="c1"># users can override this, by default this is a no-op</span>
        <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
            <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.on_predict_step_end&quot;</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">on_predict_step_end</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span></div>

<div class="viewcode-block" id="AutoUnit.on_predict_step_end"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.on_predict_step_end">[docs]</a>    <span class="k">def</span> <span class="nf">on_predict_step_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">TData</span><span class="p">,</span>
        <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="c1"># pyre-fixme[2]: Parameter annotation cannot be `Any`.</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This will be called at the end of every ``predict_step`` before returning. The user can implement this method with code to update and log their metrics,</span>
<span class="sd">        or do anything else.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: a State object which is passed from the ``predict_step``</span>
<span class="sd">            data: a batch of data which is passed from the ``predict_step``</span>
<span class="sd">            step: how many ``predict_step``s have been completed</span>
<span class="sd">            outputs: the outputs of the model forward pass</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="AutoUnit.step_lr_scheduler"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.step_lr_scheduler">[docs]</a>    <span class="k">def</span> <span class="nf">step_lr_scheduler</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        LR step method extracted to a method in case the user wants to override</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="nf">_update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Updates weights of the module, handles clip gradient norm, etc.</span>

<span class="sd">        Returns total norm of the parameter gradients, if gradient norm clipping is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
        <span class="n">grad_scaler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_scaler</span>
        <span class="c1"># Run gradient clipping, optimizer step, and zero_grad</span>
        <span class="n">clip_grad_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span>
        <span class="n">clip_grad_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_value</span>
        <span class="k">if</span> <span class="n">grad_scaler</span> <span class="ow">and</span> <span class="p">(</span><span class="n">clip_grad_norm</span> <span class="ow">or</span> <span class="n">clip_grad_value</span><span class="p">):</span>
            <span class="c1"># unscale the gradients of optimizer&#39;s assigned params in-place in preparation for gradient clipping</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.grad_unscale&quot;</span><span class="p">):</span>
                <span class="n">grad_scaler</span><span class="o">.</span><span class="n">unscale_</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="n">total_grad_norm</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># gradient norm clipping</span>
        <span class="k">if</span> <span class="n">clip_grad_norm</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_is_fsdp_module</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">FSDP</span><span class="p">):</span>
                    <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                        <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.clip_grad_norm&quot;</span>
                    <span class="p">):</span>
                        <span class="n">total_grad_norm</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                            <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_grad_norm</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Composable FSDP clip_grad_norm is not yet implemented: https://github.com/pytorch/pytorch/issues/97271&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.clip_grad_norm&quot;</span>
                <span class="p">):</span>
                    <span class="n">total_grad_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span>
                        <span class="n">parameters</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                        <span class="n">max_norm</span><span class="o">=</span><span class="n">clip_grad_norm</span><span class="p">,</span>
                    <span class="p">)</span>

        <span class="c1"># gradient value clipping</span>
        <span class="k">if</span> <span class="n">clip_grad_value</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.clip_grad_value&quot;</span>
            <span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                    <span class="n">clip_value</span><span class="o">=</span><span class="n">clip_grad_value</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.optimizer_step&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">grad_scaler</span><span class="p">:</span>
                <span class="n">grad_scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
                <span class="c1"># update the scale for next iteration</span>
                <span class="n">grad_scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># sets gradients to zero</span>
        <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
            <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.optimizer_zero_grad&quot;</span>
        <span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_lr_interval</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_lr_and_swa</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_steps_completed</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">total_grad_norm</span>

<div class="viewcode-block" id="AutoUnit.get_next_train_batch"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.get_next_train_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_next_train_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span> <span class="n">TData</span><span class="p">]:</span>
        <span class="c1"># Override the default behavior from PredictUnit in order to enable prefetching if possible.</span>
        <span class="n">pass_data_iter_to_step</span> <span class="o">=</span> <span class="n">_step_requires_iterator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pass_data_iter_to_step</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data_iter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">)</span></div>

<div class="viewcode-block" id="AutoUnit.get_next_eval_batch"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.get_next_eval_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_next_eval_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span> <span class="n">TData</span><span class="p">]:</span>
        <span class="c1"># Override the default behavior from PredictUnit in order to enable prefetching if possible.</span>
        <span class="n">pass_data_iter_to_step</span> <span class="o">=</span> <span class="n">_step_requires_iterator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pass_data_iter_to_step</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data_iter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">)</span></div>

<div class="viewcode-block" id="AutoUnit.get_next_predict_batch"><a class="viewcode-back" href="../../../framework/auto_unit.html#torchtnt.framework.auto_unit.AutoUnit.get_next_predict_batch">[docs]</a>    <span class="k">def</span> <span class="nf">get_next_predict_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Iterator</span><span class="p">[</span><span class="n">TData</span><span class="p">],</span> <span class="n">TData</span><span class="p">]:</span>
        <span class="c1"># Override the default behavior from PredictUnit in order to enable prefetching if possible.</span>
        <span class="n">pass_data_iter_to_step</span> <span class="o">=</span> <span class="n">_step_requires_iterator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_step</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pass_data_iter_to_step</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data_iter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_next_batch</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_should_update_swa</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">swa_params</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="n">swa_params</span> <span class="o">=</span> <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">swa_params</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_lr_interval</span> <span class="o">==</span> <span class="s2">&quot;step&quot;</span><span class="p">:</span>
            <span class="n">current_progress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_steps_completed</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># since num_epochs_completed is incremented prior to updating swa</span>
            <span class="n">current_progress</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_progress</span><span class="o">.</span><span class="n">num_epochs_completed</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">current_progress</span> <span class="o">&gt;=</span> <span class="n">swa_params</span><span class="o">.</span><span class="n">warmup_steps_or_epochs</span><span class="p">:</span>
            <span class="n">progress_since</span> <span class="o">=</span> <span class="n">current_progress</span> <span class="o">-</span> <span class="n">swa_params</span><span class="o">.</span><span class="n">warmup_steps_or_epochs</span>
            <span class="n">update_freq</span> <span class="o">=</span> <span class="n">swa_params</span><span class="o">.</span><span class="n">step_or_epoch_update_freq</span>
            <span class="k">return</span> <span class="n">progress_since</span> <span class="o">%</span> <span class="n">update_freq</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">_update_swa</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
            <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.stochastic_weight_avg_update&quot;</span>
        <span class="p">):</span>
            <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">swa_model</span><span class="p">)</span><span class="o">.</span><span class="n">update_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_lr_and_swa</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span> <span class="n">number_of_steps_or_epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_update_swa</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_swa</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">swa_scheduler</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">number_of_steps_or_epochs</span>
            <span class="o">&gt;=</span> <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">swa_params</span><span class="p">)</span><span class="o">.</span><span class="n">warmup_steps_or_epochs</span>
        <span class="p">):</span>
            <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.swa_lr_scheduler_step&quot;</span>
            <span class="p">):</span>
                <span class="n">none_throws</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">swa_scheduler</span><span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># optionally step lr scheduler if SWA not in use</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">get_timing_context</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.lr_scheduler_step&quot;</span>
                <span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">step_lr_scheduler</span><span class="p">()</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/js/torchtnt.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>