


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchtnt.utils.distributed &mdash; TorchTNT master documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/torchtnt.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

  
  
    <div id="redirect-banner" style="display: none">
      <p>
        ðŸŽ‰ This is the public documentation. There is internal documentation for Meta employees at
        <a href="https://www.internalfb.com/intern/staticdocs/torchtnt/">https://www.internalfb.com/intern/staticdocs/torchtnt/</a>
      </p>
    </div>
  

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (unstable)
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpointing.html">Checkpointing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Framework</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/unit.html">Unit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/auto_unit.html">AutoUnit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/eval.html">Evaluate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/predict.html">Predict</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/fit.html">Fit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/state.html">State</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework/callbacks.html">Callbacks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Utils</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../utils/utils.html">Utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchtnt.utils.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchtnt.utils.distributed</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>


<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">pyre_extensions</span> <span class="kn">import</span> <span class="n">ParameterSpecification</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">distributed</span> <span class="k">as</span> <span class="n">dist</span><span class="p">,</span> <span class="n">multiprocessing</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.distributed.elastic.utils.distributed</span> <span class="kn">import</span> <span class="n">get_free_port</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span>


<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="n">DistObjList</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">]]</span>
<span class="n">TParams</span> <span class="o">=</span> <span class="n">ParameterSpecification</span><span class="p">(</span><span class="s2">&quot;TParams&quot;</span><span class="p">)</span>
<span class="n">TReturn</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;TReturn&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="PGWrapper"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.PGWrapper.html#torchtnt.utils.distributed.PGWrapper">[docs]</a><span class="k">class</span> <span class="nc">PGWrapper</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper around ProcessGroup that allows collectives to be issued in a</span>
<span class="sd">    consistent fashion regardless of the following scenarios:</span>

<span class="sd">        pg is None, distributed is initialized:     use WORLD as pg</span>
<span class="sd">        pg is None, distributed is not initialized: single process app</span>
<span class="sd">        pg is not None:                             use pg</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="PGWrapper.__init__"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.PGWrapper.html#torchtnt.utils.distributed.PGWrapper.__init__">[docs]</a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg</span></div>

    <span class="k">def</span> <span class="nf">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">broadcast_object_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_list</span><span class="p">:</span> <span class="n">DistObjList</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">obj_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">all_gather_object</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_list</span><span class="p">:</span> <span class="n">DistObjList</span><span class="p">,</span> <span class="n">obj</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obj_list</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="n">obj_list</span><span class="p">)</span>  <span class="c1"># to make pyre happy</span>
            <span class="n">obj_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
            <span class="k">return</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_object</span><span class="p">(</span><span class="n">obj_list</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">scatter_object_list</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">output_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="kc">None</span><span class="p">],</span>
        <span class="n">input_list</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DistObjList</span><span class="p">],</span>
        <span class="n">src</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">src</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;The src rank&#39;s input_list for scatter_object_list must not be None.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The length of input_list </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> for scatter_object_list &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;must be the same as the process group&#39;s world size (</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">).&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">world_size</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">return</span>

        <span class="c1"># scatter_object_list does not yet support NCCL backend</span>
        <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">broadcast_object_list</span><span class="p">(</span><span class="n">obj_list</span><span class="o">=</span><span class="n">input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">)</span>
            <span class="n">output_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_list</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span>
            <span class="k">return</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">scatter_object_list</span><span class="p">(</span><span class="n">output_list</span><span class="p">,</span> <span class="n">input_list</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pg</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_global_rank"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_global_rank.html#torchtnt.utils.distributed.get_global_rank">[docs]</a><span class="k">def</span> <span class="nf">get_global_rank</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get rank using torch.distributed if available. Otherwise, the RANK env var instead if initialized.</span>
<span class="sd">    Returns 0 if neither condition is met.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>

    <span class="n">environ_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_rank</span><span class="o">.</span><span class="n">isdecimal</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>

    <span class="k">return</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="get_local_rank"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_local_rank.html#torchtnt.utils.distributed.get_local_rank">[docs]</a><span class="k">def</span> <span class="nf">get_local_rank</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get rank using the ``LOCAL_RANK`` environment variable, if populated: https://pytorch.org/docs/stable/elastic/run.html#environment-variables</span>
<span class="sd">    Defaults to 0 if ``LOCAL_RANK`` is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">environ_local_rank</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_local_rank</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">environ_local_rank</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span></div>


<span class="k">def</span> <span class="nf">get_local_world_size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get local world size using the ``LOCAL_WORLD_SIZE`` environment variable, if populated: https://pytorch.org/docs/stable/elastic/run.html#environment-variables</span>
<span class="sd">    Defaults to 1 if ``LOCAL_WORLD_SIZE`` is not set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">environ_local_world_size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_WORLD_SIZE&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">environ_local_world_size</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">environ_local_world_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span>


<div class="viewcode-block" id="get_world_size"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_world_size.html#torchtnt.utils.distributed.get_world_size">[docs]</a><span class="k">def</span> <span class="nf">get_world_size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get world size using torch.distributed if available. Otherwise, the WORLD_SIZE env var is used instead if initialized.</span>
<span class="sd">    Returns 1 if neither condition is met.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="n">world_size</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">world_size</span><span class="o">.</span><span class="n">isdecimal</span><span class="p">():</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">world_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="mi">1</span></div>


<div class="viewcode-block" id="barrier"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.barrier.html#torchtnt.utils.distributed.barrier">[docs]</a><span class="k">def</span> <span class="nf">barrier</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a synchronization point across all processes when using distributed.</span>
<span class="sd">    If torch.distributed is initialized, this function will invoke a barrier across the global process group.</span>
<span class="sd">    For more granular process group wrapping, please refer to :class:`~torchtnt.utils.PGWrapper`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()):</span>
        <span class="k">return</span>
    <span class="n">backend</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="n">dist</span><span class="o">.</span><span class="n">Backend</span><span class="o">.</span><span class="n">NCCL</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span></div>


<div class="viewcode-block" id="destroy_process_group"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.destroy_process_group.html#torchtnt.utils.distributed.destroy_process_group">[docs]</a><span class="k">def</span> <span class="nf">destroy_process_group</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Destroy the global process group, if one is already initialized.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">destroy_process_group</span><span class="p">()</span></div>


<div class="viewcode-block" id="get_process_group_backend_from_device"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_process_group_backend_from_device.html#torchtnt.utils.distributed.get_process_group_backend_from_device">[docs]</a><span class="k">def</span> <span class="nf">get_process_group_backend_from_device</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function that gets the default process group backend from the device.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;nccl&quot;</span> <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="s2">&quot;gloo&quot;</span></div>


<span class="k">def</span> <span class="nf">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid world_size value provided: </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">. Value must be greater than 0.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid rank value provided: </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">. Value must be greater than non-negative.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&gt;=</span> <span class="n">world_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Invalid rank and world_size values provided: rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">, world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">. Rank must be less than world_size.&quot;</span>
        <span class="p">)</span>


<div class="viewcode-block" id="get_file_init_method"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_file_init_method.html#torchtnt.utils.distributed.get_file_init_method">[docs]</a><span class="k">def</span> <span class="nf">get_file_init_method</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets init method for the TCP protocol for the distributed environment.</span>
<span class="sd">    For more information, see here: https://pytorch.org/docs/stable/distributed.html#shared-file-system-initialization</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: global number of workers. If ``None``, the default is fetched using :function:`get_world_size`.</span>
<span class="sd">        rank: Global rank of the worker calling the function. If ``None``, the default is fetched using :function:`get_global_rank`.</span>
<span class="sd">        filename: The filename to use for synchronization. If ``None``, a new temporary file is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span> <span class="k">if</span> <span class="n">world_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_global_rank</span><span class="p">()</span>
    <span class="n">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_file</span><span class="p">:</span>
            <span class="n">filename</span> <span class="o">=</span> <span class="n">tmp_file</span><span class="o">.</span><span class="n">name</span>
    <span class="n">init_method</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;file://</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">?world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&amp;rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">init_method</span></div>


<div class="viewcode-block" id="get_tcp_init_method"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.get_tcp_init_method.html#torchtnt.utils.distributed.get_tcp_init_method">[docs]</a><span class="k">def</span> <span class="nf">get_tcp_init_method</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hostname</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">port</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gets init method for the TCP protocol for the distributed environment.</span>
<span class="sd">    For more information, see here: https://pytorch.org/docs/stable/distributed.html#tcp-initialization.</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: global number of workers. If ``None``, the default is fetched using :function:`get_world_size`.</span>
<span class="sd">        rank: Global rank of the worker calling the function. If ``None``, the default is fetched using :function:`get_global_rank`.</span>
<span class="sd">        hostname: an address that belongs to the rank 0 process. If ``None``, then ``localhost`` is used.</span>
<span class="sd">        port: A free port to use for communication. If ``None``, this port is automatically selected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">world_size</span> <span class="k">if</span> <span class="n">world_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_global_rank</span><span class="p">()</span>
    <span class="n">_validate_global_rank_world_size</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
    <span class="n">host_addr</span> <span class="o">=</span> <span class="n">hostname</span> <span class="k">if</span> <span class="n">hostname</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">host_port</span> <span class="o">=</span> <span class="n">port</span> <span class="k">if</span> <span class="n">port</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">get_free_port</span><span class="p">()</span>
    <span class="n">init_method</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;tcp://</span><span class="si">{</span><span class="n">host_addr</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">host_port</span><span class="si">}</span><span class="s2">?world_size=</span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&amp;rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">init_method</span></div>


<span class="k">def</span> <span class="nf">_simple_all_gather_tensors</span><span class="p">(</span>
    <span class="n">result</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">],</span> <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">stacked_result_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">gathered_result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">stacked_result_sizes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gathered_result</span>


<div class="viewcode-block" id="all_gather_tensors"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.all_gather_tensors.html#torchtnt.utils.distributed.all_gather_tensors">[docs]</a><span class="k">def</span> <span class="nf">all_gather_tensors</span><span class="p">(</span>
    <span class="n">result</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function to gather tensors from several distributed processes onto a list that is broadcasted to all processes.</span>
<span class="sd">    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case</span>
<span class="sd">    tensors are padded, gathered and then trimmed to secure equal workload for all processes.</span>

<span class="sd">    Args:</span>
<span class="sd">        result: the value to sync</span>
<span class="sd">        group: the process group to gather results from. Defaults to all processes (world)</span>

<span class="sd">    Return:</span>
<span class="sd">        gathered_result: list with size equal to the process group where</span>
<span class="sd">            gathered_result[i] corresponds to result tensor from process i</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if torch.distributed is not available or not initialized</span>
    <span class="c1"># return single-item list containing the result</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">result</span><span class="p">]</span>

    <span class="c1"># convert tensors to contiguous format</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># if the tensor is scalar, things are easy</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_simple_all_gather_tensors</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># gather sizes of all tensors</span>
    <span class="n">local_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">stacked_local_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">local_size</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">local_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">stacked_local_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">local_size</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">local_size</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">,</span> <span class="n">local_size</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>

    <span class="c1"># if the backend is NCCL, we can gather the differently sized tensors without padding</span>
    <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">group</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span><span class="p">:</span>
        <span class="n">gathered_result</span> <span class="o">=</span> <span class="p">[</span><span class="n">result</span><span class="o">.</span><span class="n">new_empty</span><span class="p">(</span><span class="n">size</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">local_sizes</span><span class="p">]</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gathered_result</span>

    <span class="c1"># if shapes are all the same, then do a simple gather:</span>
    <span class="n">stacked_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">)</span>
    <span class="n">max_size</span> <span class="o">=</span> <span class="n">stacked_sizes</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">min_size</span> <span class="o">=</span> <span class="n">stacked_sizes</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="n">all_sizes_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">max_size</span><span class="p">,</span> <span class="n">min_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">all_sizes_equal</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_simple_all_gather_tensors</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="c1"># if not, we need to pad each local tensor to maximum size, gather and then truncate</span>
    <span class="n">pad_dims</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pad_by</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_size</span> <span class="o">-</span> <span class="n">local_size</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">pad_by</span><span class="p">):</span>
        <span class="n">pad_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pad_dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">result_padded</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">pad_dims</span><span class="p">)</span>
    <span class="n">stacked_result_padded</span> <span class="o">=</span> <span class="p">[</span><span class="n">world_size</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">result_padded</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="n">gathered_result</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">stacked_result_padded</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">result_padded</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">result_padded</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gathered_result</span><span class="p">,</span> <span class="n">result_padded</span><span class="p">,</span> <span class="n">group</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">item_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">local_sizes</span><span class="p">):</span>
        <span class="n">slice_param</span> <span class="o">=</span> <span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">item_size</span><span class="p">]</span>
        <span class="n">gathered_result</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">gathered_result</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">slice_param</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">gathered_result</span></div>


<span class="n">TReturn</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;TReturn&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="rank_zero_fn"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.rank_zero_fn.html#torchtnt.utils.distributed.rank_zero_fn">[docs]</a><span class="k">def</span> <span class="nf">rank_zero_fn</span><span class="p">(</span><span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Function that can be used as a decorator to enable a function to be called on global rank 0 only.</span>

<span class="sd">    Note:</span>
<span class="sd">        This decorator should be used judiciously. it should never be used on functions that need synchronization.</span>
<span class="sd">        It should be used very carefully with functions that mutate local state as well</span>

<span class="sd">    Example:</span>

<span class="sd">        &gt;&gt;&gt; from torchtnt.utilities.distributed import rank_zero_fn</span>
<span class="sd">        &gt;&gt;&gt; @rank_zero_fn</span>
<span class="sd">        ... def foo():</span>
<span class="sd">        ...     return 1</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = foo() # x is 1 if global rank is 0 else x is None</span>

<span class="sd">    Args:</span>
<span class="sd">        fn: the desired function to be executed on rank 0 only</span>

<span class="sd">    Return:</span>
<span class="sd">        wrapped_fn: the wrapped function that executes only if the global rank is  0</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">get_global_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">wrapped_fn</span></div>


<span class="k">class</span> <span class="nc">_BatchNormXd</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">_BatchNorm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The only difference between :class:`torch.nn.BatchNorm1d`, :class:`torch.nn.BatchNorm2d`,</span>
<span class="sd">    :class:`torch.nn.BatchNorm3d`, etc is this method that is overwritten by the sub-class.</span>
<span class="sd">    This method is used when calling forward as a sanity check.</span>
<span class="sd">    When using :function:`revert_sync_batchnorm` this sanity check is lost.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_check_input_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>


<div class="viewcode-block" id="revert_sync_batchnorm"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.revert_sync_batchnorm.html#torchtnt.utils.distributed.revert_sync_batchnorm">[docs]</a><span class="k">def</span> <span class="nf">revert_sync_batchnorm</span><span class="p">(</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to convert all :class:`torch.nn.SyncBatchNorm` layers in the module to</span>
<span class="sd">    :attr:`BatchNorm*D` layers. This function reverts :meth:`torch.nn.SyncBatchNorm.convert_sync_batchnorm`.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (nn.Module): module containing one or more :class:`torch.nn.SyncBatchNorm` layers</span>
<span class="sd">        device (optional): device in which the :attr:`BatchNorm*D` should be created,</span>
<span class="sd">                default is cpu</span>

<span class="sd">    Returns:</span>
<span class="sd">        The original :attr:`module` with the converted :attr:`BatchNorm*D`</span>
<span class="sd">        layers. If the original :attr:`module` is a :class:`torch.nn.SyncBatchNorm` layer,</span>
<span class="sd">        a new :attr:`BatchNorm*D` layer object will be returned</span>
<span class="sd">        instead. Note that the :attr:`BatchNorm*D` layers returned will not have input dimension information.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # Network with nn.BatchNorm layer</span>
<span class="sd">        &gt;&gt;&gt; module = torch.nn.Sequential(</span>
<span class="sd">        &gt;&gt;&gt;            torch.nn.Linear(20, 100),</span>
<span class="sd">        &gt;&gt;&gt;            torch.nn.BatchNorm1d(100),</span>
<span class="sd">        &gt;&gt;&gt;          ).cuda()</span>
<span class="sd">        &gt;&gt;&gt; sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module)</span>
<span class="sd">        &gt;&gt;&gt; reverted_module = revert_sync_batchnorm(sync_bn_module, torch.device(&quot;cuda&quot;))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">module_output</span> <span class="o">=</span> <span class="n">module</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">batchnorm</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="p">):</span>
        <span class="n">module_output</span> <span class="o">=</span> <span class="n">_BatchNormXd</span><span class="p">(</span>
            <span class="n">module</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">,</span>
            <span class="n">module</span><span class="o">.</span><span class="n">track_running_stats</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">module_output</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span>
                <span class="n">module_output</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">running_mean</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">running_var</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">num_batches_tracked</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_batches_tracked</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;qconfig&quot;</span><span class="p">):</span>
            <span class="n">module_output</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">qconfig</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="n">module_output</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">revert_sync_batchnorm</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">device</span><span class="p">))</span>
    <span class="k">del</span> <span class="n">module</span>
    <span class="k">return</span> <span class="n">module_output</span></div>


<div class="viewcode-block" id="sync_bool"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.sync_bool.html#torchtnt.utils.distributed.sync_bool">[docs]</a><span class="k">def</span> <span class="nf">sync_bool</span><span class="p">(</span>
    <span class="n">val</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">pg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">dist</span><span class="o">.</span><span class="n">ProcessGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">coherence_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;any&quot;</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;rank_zero&quot;</span><span class="p">],</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;any&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility to synchronize a boolean value across members of a provided process group.</span>

<span class="sd">    In the case ``torch.distributed`` is not available or initialized, the input ``val`` is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        val (bool): boolean value to synchronize</span>
<span class="sd">        pg: process group to use for synchronization. If not specified, the default process group is used.</span>
<span class="sd">        coherence_mode Union[str, int, float]: the manner in which the boolean value should be synchronized. 5 options are currently supported:</span>
<span class="sd">            1. any (default): If any rank provides a True value, all ranks should receive True.</span>
<span class="sd">            2. all: Only if all ranks provide a True value should all ranks receive True.</span>
<span class="sd">            3. rank_zero: Makes rank 0 process&#39;s value the source of truth and broadcasts the result to all other processes.</span>
<span class="sd">            4. If an integer N is provided, return True only if at least N processes provide a True value.</span>
<span class="sd">            5. If a float F is provided, return True only if at least this ratio of processes provide a True value. The ratio provided should be in the range [0, 1].</span>

<span class="sd">    Returns:</span>
<span class="sd">        The synchronized boolean value.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; val = True</span>
<span class="sd">        &gt;&gt;&gt; # synced_val is True iff all ranks provide a True value to the function</span>
<span class="sd">        &gt;&gt;&gt; synced_val = sync_bool(val, coherence_mode=&quot;all&quot;)</span>
<span class="sd">        &gt;&gt;&gt; if synced_val:</span>
<span class="sd">        &gt;&gt;&gt;     print(&quot;success&quot;)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">val</span>

    <span class="n">pg</span> <span class="o">=</span> <span class="n">pg</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">WORLD</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_backend</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;nccl&quot;</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="p">)</span>
    <span class="n">pg_wrapper</span> <span class="o">=</span> <span class="n">PGWrapper</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>
    <span class="k">if</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">256</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span>

    <span class="n">indicator</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">val</span>
        <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;rank_zero&quot;</span><span class="p">:</span>
        <span class="c1"># Broadcast from rank 0 to all other ranks</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">pg</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">indicator</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;any&quot;</span><span class="p">:</span>
        <span class="c1"># sum up the indicators across all the ranks.</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">coherence_mode</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coherence_mode</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># if &gt;= int(coherence_mode) processes signal to stop, all processes stop</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">coherence_mode</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coherence_mode</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">indicator</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">indicator</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">pg_wrapper</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())</span> <span class="o">&gt;=</span> <span class="n">coherence_mode</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;Invalid value for `coherence_mode` provided: Expected type int, float, or one of (&quot;any&quot;, &quot;all&quot;, &quot;rank_zero&quot;), but received </span><span class="si">{</span><span class="n">coherence_mode</span><span class="si">}</span><span class="s1">.&#39;</span>
        <span class="p">)</span></div>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ProcessGroupSetupParams</span><span class="p">:</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">port</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span>


<div class="viewcode-block" id="spawn_multi_process"><a class="viewcode-back" href="../../../utils/generated/torchtnt.utils.distributed.spawn_multi_process.html#torchtnt.utils.distributed.spawn_multi_process">[docs]</a><span class="k">def</span> <span class="nf">spawn_multi_process</span><span class="p">(</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">test_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">],</span>
    <span class="o">*</span><span class="n">test_method_args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="o">**</span><span class="n">test_method_kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">TReturn</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Spawn single node, multi-rank function.</span>
<span class="sd">    Uses localhost and free port to communicate.</span>

<span class="sd">    Args:</span>
<span class="sd">        world_size: number of processes</span>
<span class="sd">        backend: backend to use. for example, &quot;nccl&quot;, &quot;gloo&quot;, etc</span>
<span class="sd">        test_method: callable to spawn. first 3 arguments are rank, world_size and mp output dict</span>
<span class="sd">        test_method_args: args for the test method</span>
<span class="sd">        test_method_kwargs: kwargs for the test method</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list, l, where l[i] is the return value of test_method on rank i</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">manager</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Manager</span><span class="p">()</span>
    <span class="n">mp_output_dict</span> <span class="o">=</span> <span class="n">manager</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>

    <span class="n">port</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">get_free_port</span><span class="p">())</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
        <span class="c1"># torch.multiprocessing.spawn sends rank as the first param</span>
        <span class="c1"># https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn</span>
        <span class="n">_init_pg_and_rank_and_launch_test</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span>
            <span class="n">ProcessGroupSetupParams</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="n">port</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">),</span>
            <span class="n">mp_output_dict</span><span class="p">,</span>
            <span class="n">test_method</span><span class="p">,</span>
            <span class="n">test_method_args</span><span class="p">,</span>
            <span class="n">test_method_kwargs</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">output_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">):</span>
        <span class="n">output_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mp_output_dict</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">output_list</span></div>


<span class="k">def</span> <span class="nf">_init_pg_and_rank_and_launch_test</span><span class="p">(</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">pg_setup_params</span><span class="p">:</span> <span class="n">ProcessGroupSetupParams</span><span class="p">,</span>
    <span class="n">mp_output_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
    <span class="n">test_method</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="n">TParams</span><span class="p">,</span> <span class="n">TReturn</span><span class="p">],</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">object</span><span class="p">],</span>
    <span class="n">kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;localhost&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pg_setup_params</span><span class="o">.</span><span class="n">port</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
        <span class="n">world_size</span><span class="o">=</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">pg_setup_params</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">60</span><span class="p">),</span>  <span class="c1"># setting up timeout for distributed collectives</span>
    <span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mp_output_dict</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_method</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># pyre-fixme</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">destroy_process_group</span><span class="p">()</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/js/torchtnt.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>